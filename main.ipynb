{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b83c7976-cae0-47df-97f4-f42907432d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb0b52a-81fe-483b-b4eb-7bf7744cd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 135\n",
    "img_width = 155\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37c44cf3-0782-44f7-94a3-b0201631afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7557 files belonging to 16 classes.\n",
      "['decimal', 'div', 'eight', 'equal', 'five', 'four', 'minus', 'multiply', 'nine', 'one', 'plus', 'seven', 'six', 'three', 'two', 'zero']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'data/train',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2375a42-d09a-4ba4-8b68-435b2d22d74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1010 files belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  'data/eval', \n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75dfa9e-b96b-418a-92d1-822b300383f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08198113-b454-4f52-95aa-99b7d571b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2621cf15-60b8-44e5-b488-6124cccfbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.Input(shape=(img_height, img_width, 3)), \n",
    "  \n",
    "  layers.Rescaling(1./255),\n",
    "\n",
    "  # First convolutional block\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "\n",
    "  # Second convolutional block\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "\n",
    "  # Third convolutional block\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "\n",
    "  # Flatten the results to feed into a Dense layer\n",
    "  layers.Flatten(),\n",
    "\n",
    "  # A standard fully-connected layer\n",
    "  layers.Dense(128, activation='relu'),\n",
    "\n",
    "  # Output layer\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa7001d-980f-4b92-bee7-2fa75a7d235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca01109a-e0a3-44fa-915c-6305c0c722c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 157ms/step - accuracy: 0.6496 - loss: 1.1734 - val_accuracy: 0.8327 - val_loss: 0.5932\n",
      "Epoch 2/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 154ms/step - accuracy: 0.9067 - loss: 0.3114 - val_accuracy: 0.8960 - val_loss: 0.3595\n",
      "Epoch 3/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 151ms/step - accuracy: 0.9559 - loss: 0.1502 - val_accuracy: 0.8980 - val_loss: 0.3864\n",
      "Epoch 4/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 153ms/step - accuracy: 0.9735 - loss: 0.0784 - val_accuracy: 0.9149 - val_loss: 0.3298\n",
      "Epoch 5/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 156ms/step - accuracy: 0.9845 - loss: 0.0527 - val_accuracy: 0.9059 - val_loss: 0.3854\n",
      "Epoch 6/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 153ms/step - accuracy: 0.9847 - loss: 0.0435 - val_accuracy: 0.9228 - val_loss: 0.4410\n",
      "Epoch 7/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 159ms/step - accuracy: 0.9903 - loss: 0.0269 - val_accuracy: 0.9069 - val_loss: 0.4355\n",
      "Epoch 8/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 154ms/step - accuracy: 0.9919 - loss: 0.0246 - val_accuracy: 0.9099 - val_loss: 0.5258\n",
      "Epoch 9/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 155ms/step - accuracy: 0.9906 - loss: 0.0313 - val_accuracy: 0.9139 - val_loss: 0.4164\n",
      "Epoch 10/10\n",
      "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 154ms/step - accuracy: 0.9976 - loss: 0.0096 - val_accuracy: 0.9267 - val_loss: 0.4102\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa9f4ad-348c-423b-a5b9-b81075b9293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "146eaa29-0b0c-43ce-98b0-806fd54d2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "image_path = \"input_sample.png\"\n",
    "output_folder = \"characters\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "resize_dim = (img_width, img_height)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Load & preprocess\n",
    "# -----------------------------\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if img is None:\n",
    "    raise ValueError(\"Image not found. Check path.\")\n",
    "\n",
    "_, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "# Remove small noise\n",
    "kernel = np.ones((2,2), np.uint8)\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Horizontal dilation\n",
    "# -----------------------------\n",
    "# Connect vertically separated parts like ÷ or +, so they don't split\n",
    "hor_kernel = np.ones((1, 5), np.uint8)\n",
    "thresh_hor = cv2.dilate(thresh, hor_kernel, iterations=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Find vertical cuts (only between digits)\n",
    "# -----------------------------\n",
    "vertical_sum = np.sum(thresh_hor, axis=0)  # sum along rows\n",
    "threshold = 5  # adjust depending on image\n",
    "\n",
    "cuts = []\n",
    "start = None\n",
    "for i, val in enumerate(vertical_sum):\n",
    "    if val > threshold and start is None:\n",
    "        start = i\n",
    "    elif val <= threshold and start is not None:\n",
    "        end = i\n",
    "        if end - start > 2:\n",
    "            cuts.append((start, end))\n",
    "        start = None\n",
    "if start is not None:\n",
    "    cuts.append((start, len(vertical_sum)))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Crop & save each character\n",
    "# -----------------------------\n",
    "char_count = 0\n",
    "for (x_start, x_end) in cuts:\n",
    "    char_img = thresh[:, x_start:x_end]\n",
    "    char_img = cv2.bitwise_not(char_img)\n",
    "    \n",
    "    # Maintain aspect ratio with padding\n",
    "    h, w = char_img.shape\n",
    "    \n",
    "    # Create a square canvas with padding\n",
    "    max_dim = max(h, w)\n",
    "    square_img = np.ones((max_dim, max_dim), dtype=np.uint8) * 255  # white background\n",
    "    \n",
    "    # Center the character\n",
    "    y_offset = (max_dim - h) // 2\n",
    "    x_offset = (max_dim - w) // 2\n",
    "    square_img[y_offset:y_offset+h, x_offset:x_offset+w] = char_img\n",
    "    \n",
    "    # Now resize the square to model dimensions\n",
    "    char_img_resized = cv2.resize(square_img, (img_width, img_height))\n",
    "    char_img_rgb = cv2.cvtColor(char_img_resized, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    char_count += 1\n",
    "    cv2.imwrite(f\"{output_folder}/char_{char_count}.png\", char_img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3152cac-5be1-45d3-adc7-60ac0878786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "Character 1: nine (confidence: 1.00)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Character 2: multiply (confidence: 1.00)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Character 3: eight (confidence: 1.00)\n",
      "\n",
      "Expression: 9 × 8\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Process each saved character\n",
    "predictions = []\n",
    "for i in range(1, char_count + 1):\n",
    "    # Load image\n",
    "    img = tf.keras.utils.load_img(\n",
    "        f\"{output_folder}/char_{i}.png\",\n",
    "        target_size=(img_height, img_width),\n",
    "        color_mode='rgb'\n",
    "    )\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(prediction)]\n",
    "    confidence = tf.nn.softmax(prediction[0])[np.argmax(prediction)]\n",
    "    \n",
    "    predictions.append(predicted_class)\n",
    "    print(f\"Character {i}: {predicted_class} (confidence: {confidence:.2f})\")\n",
    "\n",
    "symbol_map = {\n",
    "    'five': '5', 'div': '÷', 'three': '3', \n",
    "    'plus': '+', 'minus': '-', 'multiply': '×',\n",
    "    'equal': '=', 'zero': '0', 'one': '1',\n",
    "    'two': '2', 'four': '4', 'six': '6',\n",
    "    'seven': '7', 'eight': '8', 'nine': '9',\n",
    "    'decimal': '.'\n",
    "}\n",
    "\n",
    "result = ' '.join([symbol_map.get(p, p) for p in predictions])\n",
    "print(f\"\\nExpression: {result}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd694e-7e2a-429e-956c-04b4d83816a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
